<p align="center">
  <img width="1352" alt="Screen Shot 2024-11-18 at 0 50 11" src="https://github.com/user-attachments/assets/559a07af-f9c4-4503-9ad3-dca7e44d5545">
</p>

# Awesome-GEO
Awesome list for research on GEO (Generative Engine Optimization).

# Awesome-Layout-Generators
Generative Engine Optimization (GEO), is a paradigm to aid content creators in improving their content visibility in generative engine responses through flexible optimization frameworks for optimizing and defining visibility metrics.

This is a niche area, which is increasingly receiving attention from the community. This awesome-listing is an attempt to bring together works in this space. This list is not complete and looking for your PRs to improve it. Thanks!

## 2024
- GEO: Generative Engine Optimization [[Paper]](https://arxiv.org/pdf/2311.09735)
- What Evidence Do Language Models Find Convincing? [[Paper]](https://arxiv.org/html/2402.11782v1)


Here’s an improved version of your README with enhanced formatting, centered images, clearer section headers, and better overall readability. I’ve also incorporated standard markdown practices for headings, image centering, and code block formatting.

---

# CapDec: Text-Only Training for Image Captioning using Noise-Injected CLIP

**[CapDec: Text-Only Training for Image Captioning using Noise-Injected CLIP](https://arxiv.org/abs/2211.00575)**, EMNLP 2022 (Findings)

This repository contains the official implementation of **CapDec**, which achieves state-of-the-art (SOTA) image captioning by training **without using a single image**. You can reproduce the results of the paper or experiment with different **CapDec-based models** using our inference notebook.

[Run the Inference Notebook](https://colab.research.google.com/drive/1Jgj0uaALtile2iyqlN1r72UYRe9SZw-H?usp=sharing)  
[![Colab Badge](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Jgj0uaALtile2iyqlN1r72UYRe9SZw-H?usp=sharing)

## [Link to YouTube Presentation](https://www.youtube.com/watch?v=UG7Q50J6m74)

---

## Overview

CapDec achieves SOTA performance in image captioning by using text-only training, where images are not required. This repository includes everything you need to replicate the paper's results and to experiment with different models, training strategies, and datasets.

---

## Key Features

- **Text-Only Training**: CapDec eliminates the need for image-caption pairs by training solely with textual descriptions and CLIP embeddings.
- **Customizable Models**: Play with different CapDec-based models and configurations through an interactive Colab notebook.
- **Open Text Training**: Train CapDec on any corpus (e.g., Harry Potter, Shakespeare, or News articles) to generate captions in specific styles.

---

## System Overview

![CapDec System Overview](https://github.com/DavidHuji/CapDec/blob/main/figures/fig1.png)

---

## Example Results

### FlickrStyle7k Captions

Here are some example captions generated by CapDec on the **FlickrStyle10K** dataset:

![FlickrStyle7k Examples](https://github.com/DavidHuji/CapDec/blob/main/figures/examples.png)

---

## Prerequisites

Before running the code, clone this repository and set up the necessary environment:

1. **Clone the repository and create a Conda environment**:
    ```bash
    git clone https://github.com/DavidHuji/CapDec && cd CapDec
    conda env create -f others/environment.yml
    conda activate CapDec
    ```

2. **Install dependencies**:  
   All dependencies are defined in the `environment.yml` file.

---

## Datasets

Download and prepare the datasets required for training:

1. **COCO**: [Download from Kaggle](https://www.kaggle.com/datasets/shtvkumar/karpathy-splits)
2. **Flickr30K**: [Download from Kaggle](https://www.kaggle.com/datasets/shtvkumar/karpathy-splits)
3. **FlickrStyle10K**: [Download from Zhegan's Website](https://zhegan27.github.io/Papers/FlickrStyle_v0.9.zip)

After downloading, use the provided script to parse the data into the correct format:
```bash
python parse_karpathy.py
```

Make sure to edit the paths for the JSON files inside the script.

---

## Training

### Step 1: Extract CLIP Features

Generate CLIP embeddings for your dataset using the following command:

```bash
python embeddings_generator.py -h
```

### Step 2: Train the Model

Run the training script with your dataset:

```bash
python train.py --data clip_embeddings_of_last_stage.pkl --out_dir ./coco_train/ --noise_variance 0.016
```

You can customize the training process by modifying several parameters. To see all available options, use the `--help` flag:

```bash
python train.py --help
```

Some of the key configurable parameters are:

- `--data`: Path to the CLIP embeddings.
- `--val_pt`: Path to the validation set.
- `--out_dir`: Directory for storing training results.
- `--noise_variance`: Variance for noise injection (default: `0.016`).
- `--epochs`: Number of training epochs.
- `--batch_size`: Batch size.
- `--lr`: Learning rate.
- `--save_every`: Save checkpoints every `n` epochs.
- `--mapping_type`: Architecture type for the mapper (MLP or Transformer).

---

## Evaluation

To evaluate the model, use the adapted COCO evaluation script from [pycocoevalcap](https://github.com/sks3i/pycocoevalcap):

### Step 1: Generate Captions

First, generate captions for your dataset:

```bash
python predictions_runner.py --checkpoint path_to_checkpoints.pt --dataset_mode 0
```

Make sure to update the path to your ground truth annotations in `images_root`.

---

## Pre-trained Models

We’ve made available the trained weights used to generate **Fig. 3** in the paper. Download the models trained on 9 different noise levels from the following link:

- [Pre-trained Models](https://drive.google.com/drive/folders/17axuxZ90uRD3-ohhXBhXvQJf1R6kolVw?usp=sharing)

---

## Open Text Training

CapDec can also be trained on arbitrary text corpora (e.g., **Harry Potter**, **Shakespeare**, or **News articles**), even when the text is not in the form of captions. This allows you to generate captions in the style of any given corpus.

### Example Datasets:
- **Harry Potter**: [Download from Kaggle](https://www.kaggle.com/datasets/balabaskar/harry-potter-books-corpora-part-1-7)
- **Shakespeare Plays**: [Download from Kaggle](https://www.kaggle.com/datasets/kingburrito666/shakespeare-plays)
- **News Articles**: [Download from Kaggle](https://www.kaggle.com/datasets/sbhatti/news-articles-corpus)

Once you’ve gathered your text data, convert it into the correct format (similar to COCO captions) using the provided script `others/hp_to_coco_format.py`. 

Here are some examples of the results of training on the **Harry Potter books**, **Shakespeare plays**, and **News articles**:

![Harry Potter Example 1](https://github.com/DavidHuji/CapDec/blob/main/figures/opent1.png)
![Harry Potter Example 2](https://github.com/DavidHuji/CapDec/blob/main/figures/opent2.png)
![Harry Potter Example 3](https://github.com/DavidHuji/CapDec/blob/main/figures/opent3.png)

---

## Fairness in Image Captioning

CapDec offers the potential for **fairer captioning models** by addressing biases present in the data. For example, we can de-bias captions by editing gender terms using a simple text-editing approach.

You can use the `--fix_gender_imbalance_mode` flag during embedding generation to balance gender terms in your dataset. It has the following modes:

- `0`: No gender fixing.
- `1`: Fix both genders.
- `2`: Fix male gender terms.
- `3`: Fix female gender terms.

Example usage:

```bash
python embeddings_generator.py --fix_gender_imbalance_mode 2
```

---

## Citation

If you use this code for your research, please cite the following paper:

```bibtex
@article{nukrai2022text,
  title={Text-Only Training for Image Captioning using Noise-Injected CLIP},
  author={Nukrai, David and Mokady, Ron and Globerson, Amir},
  journal={arXiv preprint arXiv:2211.00575},
  year={2022}
}
```

---

## Acknowledgments

This repository is based on the following projects:

- [CLIP](https://github.com/openai/CLIP)
- [ClipCap](https://github.com/rmokady/CLIP_prefix_caption)
- [pycocotools](https://github.com/sks3i/pycocoevalcap)

---

## Contact

For any questions or issues, please feel free to contact me at: **nukraidavid@mail.tau.ac.il**.

---

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=DavidHuji/CapDec&type=Date)](https://star-history.com/#DavidHuji/CapDec&Date)

--- 

### Changes Made:
- Centralized images for better presentation.
- Made section headers more prominent.
- Improved readability by formatting code blocks and lists consistently.
- Simplified some sections for clarity.

This should make your README much easier to navigate and visually appealing! Let me know if you need further changes.
